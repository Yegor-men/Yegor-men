# What is this?
All projects listed here are (as of now at the very least) pretty much entirely self taught. I'd like to see how far this can be taken, how much can be learnt, recreated and (hopefully) invented/created without formal education.

# My projects

## Currently working on:
- [BPOT (Back Propagation Over Time)](https://github.com/Yegor-men/BPOT-Back-Propagation-Over-Time) - The idea that a neuron layer can pass gradients upstream to the previous neurons - a biologically plausible BPTT alternative.
- [Sinlge Lifetime Learning (SLL)](https://github.com/Yegor-men/SLL) - Models which train in a single lifetime (SNNs, RTRL/e-prop, R-STDP, etc.)

## On hold / soon to start:
- [Small LMs](https://github.com/Yegor-men/Small-LMs) - Small language models of various architectures but recreated with my own code from scratch
- [SMRL](https://github.com/Yegor-men/Surprise-Minimization-RL) - The idea that RL models can be taught via surprise mimimization, lowering the difference between the expected and observed outcomes of actions in some environment
- [replAI](https://github.com/Yegor-men/replAI) - A framework for naturally texting with an LLM that can chain messages, respond, see time delays, images, send them, creating a maximally "human" interaction

## Future projects:
- [Diffusions](https://github.com/Yegor-men/Diffusions) - Various diffusion based models of various arhcitectures but recreated with my own code from scratch

## Completed (I will not be going back to this / very little updates)
- [17/06/2025] [Scale Invariant Probabilistic Neurons](https://github.com/Yegor-men/Scale-Invariant-Probabilistic-Neurons) - A silly test on scale invariant networks (a state of criticality). Done by having the synapse strength represent the probability that the postsynaptic neuron j fires given that the presynaptic neuron fires.
- [12/05/2025] [Cyber Biological Neurons](https://github.com/Yegor-men/cyber_biological_neurons) - A whacky idea of mine to recreate how biological neurons work but make them digital instead
- [06/04/2025] [GPT2 recreation](https://github.com/Yegor-men/gpt2) - A recreation of the GPT2 architecture, largely aiming to recreate the attention mechanism from scratch. Once I figured the transformer blocks were working I moved to [Small LMs](https://github.com/Yegor-men/Small-LMs) for cleaner and more refined implementations.
- [29/03/2025] [Semantic de-duplication](https://github.com/Yegor-men/Semantic-De-duplication) - Detecting and removing mass duplicates from databases
- [22/02/2023] [Transformer Stock Trader](https://github.com/Yegor-men/Transformer-Stock-Trader) - A transformer based model to try predict stock prices. Just a simple test for feasibility. A quick test reveals its not so simple.
- [11/02/2025] [Tic Tac Toe with reinfocement learning](https://github.com/Yegor-men/tic-tac-toe-rl) - A model that can play Tic Tac Toe learnt entirely through reinforcement learning
- [10/02/2025] [Variable autoencoder](https://github.com/Yegor-men/vae) - Various VAE architectures, largely aiming to recreate the Stable Diffusion VAEs but also wanting to create a 16x height/width reduction vae to be used on 2MP+ images with fast inference and accurate reconstruction. While I created AEs, I'm yet to create a VAE, I'll get back to this once trying to create diffusion models.
- [08/02/2025] [Iris classification](https://github.com/Yegor-men/iris-classification) - A model trained on the Iris classification dataset
- [08/02/2025] [MNIST](https://github.com/Yegor-men/mnist) - A series of models trained on the MNIST type datasets
- [07/02/2025] [Harmonic Loss paper](https://github.com/Yegor-men/harmonic-loss) - My implementation of the harmonic loss paper and other experiments based on using euclidean distance as probabilities.
- [20/01/2025] [Pytorch learning course made by Daniel Bourke](https://github.com/Yegor-men/learning-pytorch-from-daniel-bourke) - My repo for following along with Daniel Bourke's pytorch learning course.
- [19/02/2024] [Neural nets in raw python](https://github.com/Yegor-men/raw-python-neural-nets) - My earliest project where I made neural nets without libraries and with only numpy.

# Contact
I am open to collaboration, you can contact me via

Email (yegor.mn@gmail.com) | [LinkedIn](https://www.linkedin.com/in/yegor-menovchshikov-313150350/) | [Twitter / X](https://x.com/Yegor_Men) | [Bluesky](https://bsky.app/profile/yegormen.bsky.social)

<!--
**Yegor-men/Yegor-men** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
